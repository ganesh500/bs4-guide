{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup Guide\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from bs4 import BeautifulSoup \n",
    "'''\n",
    "BeautifulSoup beautifies html content, \n",
    "and basically turns html tags into python objects\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing the files from webpage.\n",
    "#say we have a file called home.html\n",
    "\n",
    "#opens a (in our case hypothetical) file, 'r' stands for read, \"html_file\" will be the name we give to this file\n",
    "with open(\"home.html\", \"r\") as html_file:\n",
    "    content = html_file.read()\n",
    "    #print(content) just prints all the html\n",
    "    soup = BeautifulSoup(content, 'lxml') #creates instance of beautifulsoup library, 1st argument is the file that's been read, 2nd is parser method 'lxml'\n",
    "    #print(soup.prettify) allows us to visualize HTML in a nicer way\n",
    "    tag = soup.find('h5') #searches for the 1st element of specific HTML tag in the page we're looking at, and stops the execution after the first element (html tag)\n",
    "    tags = soup.find_all('h5') #does the same and finds all of the h5 tags. This variable actually has a list, which we can iterate through\n",
    "    #print(tags)\n",
    "    for content in tags:\n",
    "        print(content.text) #prints the content for each element (tag) in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#program that goes through specific html tags, and extracts some info from children tags. Ex:) going through 3 <div> tags and extracting a price from each div tag's child <a> tag\n",
    "with open(\"home.html\", \"r\") as html_file:\n",
    "    content = html_file.read()\n",
    "    soup = BeautifulSoup(content, 'lxml') \n",
    "    tags_with_class = soup.find_all('div', class_='anyclass') #say you just want certain div tags, filter with class html attribute with 2nd parameter\n",
    "    for content in tags_with_class:\n",
    "        content = content.specifictag.text #sets a new variable equal to the text of a specific tag in the html. Example: <h5> tag children of the <div> tags in the list\n",
    "        sub_content = content.anothertag.text #does the same, in this case equal to a child tag that has text we want to print. Example: <a> tag children of the <div> tags \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"home.html\", \"r\") as html_file:\n",
    "    content = html_file.read()\n",
    "    soup = BeautifulSoup(content, 'lxml') \n",
    "    tags_with_class = soup.find_all('div', class_='anyclass')\n",
    "    for content in tags_with_class:\n",
    "        content = content.specifictag.text\n",
    "        #Now say that anothertag has some text, and we need just the last word/info, like a price. We can split the text into indices and index from the last element (located at -1).\n",
    "        sub_content = content.anothertag.text.split()[-1] \n",
    "        #We can display this summarized info in a smooth manner using an 'f string,' which embeds expression into string literal.\n",
    "        print(f'{content} costs {sub_content}') #Given that 'content' describes a some online course, and 'sub_content' represents the price\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Mini Project\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scraping real websites for jobs. \n",
    "This program pulls the latest published job from a website.\n",
    "- Request URL\n",
    "- Instantiate of BeautifulSoup Class / create object\n",
    "- variable set up for HTML tags\n",
    "- All set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Request the html text from this url\n",
    "html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=').text\n",
    "\n",
    "#instantiate the BeautifulSoup class\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "#If you go into inspect, there's an unordered list of all the jobs, listed with an <li> tag, and they each have a class. We'll reference both.\n",
    "#We want to use just the first job we are given from the search, so just use soup.find()\n",
    "job = soup.find('li', class_='clearfix job-bx wht-shd-bx') #finds the tag that has all our first job information\n",
    "company_name = job.find('h3', class_=\"joblist-comp-name\").text.replace(' ', '') #using job object we created to find it's first sub h3 tag. 'Replace' gets rid of whitespace, replaces it with nothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what HTML element has the skills required for the job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "rest, python, security, debugging\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The <span> tag has one skill in its text, and a child <strong> tag containing more skills in that tag's text. We just need the text of the whole span.\n",
    "skills = job.find('span', class_='srp-skills').text.replace(' ','') #get rid of whitespace\n",
    "skills = skills.replace(',',', ') #make it nicer\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posted few days ago\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now to find the published date.\n",
    "Remember you can't just use the text attribute on this because a span is an inline element, it doesn't simply have text like a div.\n",
    "The published date will be used for functionality. It will force our code to output just the results posted 'a few days ago' (keeping it recent)\n",
    "'''\n",
    "published_date = job.find('span', class_ = 'sim-posted').span.text\n",
    "print(published_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want the jobs just from the FIRST page (the recent ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over each job on the page regardless of date published\n",
    "jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "\n",
    "for job in jobs:\n",
    "    published_date = job.find('span', class_ = 'sim-posted').span.text\n",
    "    if 'few' in published_date:\n",
    "        company_name = job.find('h3', class_=\"joblist-comp-name\").text.replace(' ', '') \n",
    "        skills = (job.find('span', class_='srp-skills').text.replace(' ','')).replace(',', ', ') #get rid of whitespace \n",
    "\n",
    "        print(f'''\n",
    "        Company Name: {company_name}\n",
    "        Required Skills: {skills}\n",
    "        ''')\n",
    "\n",
    "        print('')\n",
    "\n",
    "#we placed the published date first so we can end the loop if it isn't few days ago "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALWAYS PRINT THE HTML YOU ARE PULLING TO SEE WHAT METHODS YOU WILL HAVE TO USE TO BEAUTIFY IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now for more features.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Say we want to allow a user to input skills they want to FILTER from their job search\n",
    "- We can accomplish this through a conditional statement that checks whether the unfamiliar skill is in the list of skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input skill(s) you are not familiar with:\n",
      "Filtering out git\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "#Allows user to input unfamiliar skill(s)\n",
    "print('Input skill(s) you are not familiar with:')\n",
    "unfamiliar_skills = input('>')\n",
    "print(f'Filtering out {unfamiliar_skills}')\n",
    "unfamiliar_skills = (unfamiliar_skills.replace(',', '')).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#goes through every job on the webpage and finds the most recent ones, denoted by 'few days ago' in the HTML\n",
    "html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation=').text\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "jobs = soup.find_all('li', class_='clearfix job-bx wht-shd-bx')\n",
    "    \n",
    "#enumerate allows us to iterate through the jobs, but gives an index (a count) to each job. Example: First file job is \n",
    "for index, job in enumerate(jobs):\n",
    "    published_date = job.find('span', class_ = 'sim-posted').span.text\n",
    "    if 'few' in published_date:\n",
    "\n",
    "        company_name = job.find('h3', class_=\"joblist-comp-name\").text.replace(' ', '') \n",
    "        skills = (job.find('span', class_='srp-skills').text.replace(' ','')).replace(',', ', ') #gets rid of whitespace \n",
    "        more_info = job.header.h2.a['href']\n",
    "        # the above line uses a bunch of tag attributes to find the link where we can find more info about the job.\n",
    "        # Since the URL is not text, we need to grab it from the 'href' attribute of the <a> tag through the above technique\n",
    "        \n",
    "        for unfamiliar_skill in unfamiliar_skills:\n",
    "            if unfamiliar_skill not in skills:\n",
    "                print(f\"Company Name: {company_name.strip()}\") #remove whitespace at beginning/end of string\n",
    "                print(f\"Required Skills: {skills.strip()}\")\n",
    "                print(f\"More Info: : {more_info}\")\n",
    "                print(f\"{published_date}\")\n",
    "\n",
    "                print('')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
